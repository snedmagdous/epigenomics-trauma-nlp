"""
preprocessing.py

Purpose:
- Processes raw PDF articles to prepare for downstream analysis.
- Cleans, tokenizes, and categorizes text based on predefined categories (e.g., Mental Health, Epigenetics).
- Extracts relevant terms and calculates statistics like co-occurrences, Jaccard similarity, etc.

Key Steps:
1. Load expanded terms for predefined categories from a JSON file (`top_similar_terms.json`).
2. Extract text content from articles (supports plain text and PDFs).
3. Preprocess text:
   - Clean and tokenize text.
   - Optionally use SciSpacy for domain-specific lemmatization and biomedical entity extraction.
4. Categorize text:
   - Match terms to predefined categories and count frequencies.
   - Compute co-occurrence and Jaccard similarity between categories.
5. Save results to a structured JSON file for further analysis.

Inputs:
- Directory containing raw articles (plain text or PDFs).
- JSON file of expanded terms (`top_similar_terms.json`) generated by `fetch.py`.

Outputs:
- JSON file (`preprocessed_articles.json`) containing:
  [
      {
          "paper_name": "example.pdf",
          "cleaned_text": "processed text here",
          "term_counts": {
              "Mental Health Terms": {"depression": 5, "anxiety": 3, ...},
              "Epigenetic Terms": {"methylation": 4, ...},
              ...
          },
          "jaccard_similarity": {
              "('Mental Health Terms', 'Epigenetic Terms')": 0.5,
              ...
          }
      },
      ...
  ]

How to Use:
- Run this script after `fetch.py` to preprocess raw article text or PDFs.
- The output JSON file can then be used in `topic_modeling.py` for deeper analysis.
"""

import os
import re
import json
import logging
import fitz  # PyMuPDF for PDF text extraction
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import download
from collections import defaultdict, Counter
import spacy

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Download NLTK dependencies
download("punkt")
download("stopwords")

# Load SciSpacy's biomedical model
nlp = spacy.load("en_core_sci_lg")

# Paths and directories
RAW_ARTICLES_DIR = "./data/papers"  # Directory with raw text or PDFs
TOP_TERMS_FILE = "./data/top_similar_terms.json"  # Terms from fetch.py
OUTPUT_FILE = "./data/preprocessed_articles.json"

# Load expanded terms (core + similar terms)
with open(TOP_TERMS_FILE, "r", encoding="utf-8") as file:
    expanded_terms = json.load(file)


def extract_text_from_pdf(pdf_path):
    """
    Extract text from PDF files.
    Args:
        pdf_path (str): File containing PDF content.
    Returns:
        list: List of dictionaries with extracted content.
    """
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        return text.strip()
    except Exception as e:
        logging.error(f"Error extracting text from {pdf_path}: {e}")
        return ""


def clean_text(text):
    """
    Perform basic text cleaning and tokenization.
    Args:
        text (str): Raw text to clean.
    Returns:
        str: Cleaned text.
    """
    return re.sub(r"[^\w\s]", " ", text.lower())


def lemmatize_and_process(text):
    """
    Lemmatize text using SciSpacy and preserve biomedical entities.
    Args:
        text (str): Cleaned text to process.
    Returns:
        list: Lemmatized tokens.
    """
    doc = nlp(text)
    return [token.lemma_ if token.ent_type_ == "" else token.text for token in doc]


def categorize_terms(text, expanded_terms):
    """
    Categorize text using expanded terms and count term frequencies.
    Args:
        text (str): Processed text to analyze.
        expanded_terms (dict): Dictionary of core terms and their expanded similar terms.
    Returns:
        dict: Term counts for each category.
    """
    term_counts = {category: Counter() for category in expanded_terms.keys()}
    tokens = text.split()
    for category, terms in expanded_terms.items():
        term_counts[category] = Counter(t for t in tokens if t in terms)
    return term_counts


def compute_co_occurrence(term_counts):
    """Compute a co-occurrence matrix for terms across categories."""
    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))
    for category1, terms1 in term_counts.items():
        for category2, terms2 in term_counts.items():
            if category1 != category2:
                for term1 in terms1:
                    for term2 in terms2:
                        co_occurrence_matrix[category1][term2] += terms1[term1]
    return co_occurrence_matrix


def preprocess_articles(input_dir=RAW_ARTICLES_DIR, expanded_terms=expanded_terms):
    """
    Process raw articles (text or PDFs) to clean, categorize, and calculate statistics.
    Args:
        input_dir (str): Directory containing raw articles.
        expanded_terms (dict): Dictionary with core terms and expanded similar terms.
    Returns:
        list: List of processed article metadata.
    """
    processed_articles = []
    global_term_counts = {category: Counter() for category in expanded_terms.keys()}
    global_relationships = defaultdict(lambda: {"terms": Counter(), "co_occurrence_count": 0, "jaccard_similarity": 0.0})

    for file_name in os.listdir(input_dir):
        file_path = os.path.join(input_dir, file_name)
        logging.info(f"Processing file: {file_name}")

        if file_name.endswith(".pdf"):
            raw_text = extract_text_from_pdf(file_path)
        else:
            with open(file_path, "r", encoding="utf-8") as file:
                raw_text = file.read()

        if not raw_text:
            continue

        cleaned_text = clean_text(raw_text)
        lemmatized_text = " ".join(lemmatize_and_process(cleaned_text))
        term_counts = categorize_terms(lemmatized_text, expanded_terms)

        # Update global term counts
        for category, counts in term_counts.items():
            global_term_counts[category].update(counts)

        # Compute co-occurrence matrix
        co_occurrence_matrix = compute_co_occurrence(term_counts)

        # Add metadata (optional step based on text heuristics)
        disparity_metadata = {
            "ethnicity": next((term for term in expanded_terms["Ethnographic Terms"] if term in lemmatized_text), "Unknown"),
            "socioeconomic_status": next((term for term in expanded_terms["Socioeconomic Terms"] if term in lemmatized_text), "Unknown"),
        }

        processed_articles.append({
            "paper_name": file_name,
            "cleaned_text": lemmatized_text,
            "term_counts": term_counts,
            "co_occurrence_matrix": co_occurrence_matrix,
            "disparity_metadata": disparity_metadata
        })

    # Calculate Jaccard similarities and global relationships
    for category1, terms1 in global_term_counts.items():
        for category2, terms2 in global_term_counts.items():
            if category1 != category2:
                intersection = sum((terms1 & terms2).values())
                union = sum((terms1 | terms2).values())
                global_relationships[(category1, category2)]["jaccard_similarity"] = intersection / union if union > 0 else 0.0
                global_relationships[(category1, category2)]["co_occurrence_count"] += intersection

    # Save processed data
    with open(OUTPUT_FILE, "w") as output_file:
        json.dump({
            "papers": processed_articles,
            "global_summary": {
                "total_term_counts": {k: dict(v) for k, v in global_term_counts.items()},
                "top_relationships": global_relationships
            }
        }, output_file, indent=4)

    logging.info(f"Processed articles saved to {OUTPUT_FILE}")



if __name__ == "__main__":
    processed_articles = preprocess_articles()
    logging.info(f"Processed {len(processed_articles)} articles successfully.")
