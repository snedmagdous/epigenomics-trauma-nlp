"""
This script expands predefined terms across categories like mental health, epigenetics, ethnicity, and socioeconomic status using Wikipedia and NLP techniques.

Key Features:
1. **Term Expansion**:
   - Fetches related terms from Wikipedia and computes semantic similarities using the SentenceTransformer model.
   - Filters and processes terms to ensure relevance and uniqueness.

2. **Categorization**:
   - Expands and organizes terms for predefined categories: mental health, epigenetics, socioeconomic status, and ethnicity.

3. **Output**:
   - Saves expanded terms to `expanded_terms.json` for downstream analysis.

Applications:
Useful for meta-analysis, text mining, and exploring relationships between social trauma, epigenetics, and mental health.
"""

import wikipediaapi
from sentence_transformers import SentenceTransformer, util  # Ensure this is installed for embeddings
import wikipediaapi, logging, re, json, numpy as np

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load the SentenceTransformer model
model = SentenceTransformer("all-MiniLM-L6-v2")
logging.info("SentenceTransformer model loaded successfully.")

# Initialize Wikipedia API with a proper user-agent
wiki_wiki = wikipediaapi.Wikipedia(
    language="en",
    user_agent="majd (ma798@cornell.edu)"
)


def is_valid_title(title):
    """
    Filter Wikipedia titles to exclude irrelevant or non-article entries.
    - Returns False for titles starting with "Help:", "Category:", etc.
    - Ensures titles contain only alphanumeric characters and spaces.

    Args:
        title (str): The title of a Wikipedia page.

    Returns:
        bool: Whether the title is valid for inclusion in the corpus.
    """
    # Exclude specific namespaces like "Help:", "Category:", etc.
    if re.match(r"^(Help:|Category:|File:|Portal:|Template:|Wikipedia:)", title):
        return False
    # Allow only alphanumeric titles and spaces
    if not re.match(r"^[A-Za-z0-9\s\-]+$", title):
        return False
    return True


def fetch_wikipedia_corpus(input_terms, max_depth=1, max_pages=200):
    """
    Dynamically fetch a set of related terms from Wikipedia based on input terms.
    - Traverses links from the initial pages up to `max_depth`.
    - Limits the number of pages fetched to `max_pages`.

    Args:
        input_terms (list of str): List of terms to search on Wikipedia.
        max_depth (int): Depth of link traversal.
        max_pages (int): Maximum number of pages to fetch.

    Returns:
        list: A list of valid Wikipedia page titles.
    """
    corpus = set() # set to ensure unique titles

    def fetch_links(page, depth):
        if depth > max_depth or len(corpus) >= max_pages:
            return
        for title in page.links.keys():
            if is_valid_title(title) and title not in corpus:
                # Add title to corpus if valid and not already added
                corpus.add(title)
                next_page = wiki_wiki.page(title)
                if next_page.exists():
                    # Recursively fetch links from the next page
                    fetch_links(next_page, depth + 1)

    # Iterate through each input term and fetch its associated links
    for term in input_terms:
        page = wiki_wiki.page(term)
        if page.exists():
            # Add the page title if valid
            if is_valid_title(page.title):
                corpus.add(page.title)
            # Fetch links starting at depth 1
            fetch_links(page, depth=1) 

    logging.info(f"Fetched {len(corpus)} pages from Wikipedia for terms: {input_terms}")
    return list(corpus)  # Convert the set to a list

def generate_similar_terms(term_list, model, corpus, topn=50, per_term=False, mock_encode=None):
    """
    Generate expanded terms by computing semantic similarity between input terms and a corpus.
    Supports both aggregated similarity and per-term similarity.

    Args:
        term_list (list of str): Initial list of input terms.
        model (SentenceTransformer): Pre-trained SentenceTransformer model.
        corpus (list of str): List of potential terms to expand into.
        topn (int): Number of top similar terms to return.
        per_term (bool): If True, compute top similar terms for each individual term in the list.
        mock_encode (callable): Mock function for encoding terms (used for testing).

    Returns:
        dict or list: If per_term is True, returns a dictionary of top terms per input term.
                      Otherwise, returns a single list of top `topn` similar terms.
    """
    logging.info(f"Generating terms for input list: {term_list}")

    # Use mock_encode for testing, otherwise model.encode
    encode_fn = mock_encode if mock_encode else model.encode

    # Encode the corpus terms
    corpus_embeddings = [encode_fn(term) for term in corpus]
    valid_corpus = corpus

    logging.info(f"Valid corpus size: {len(valid_corpus)}")

    # If per-term similarity is required, process each term individually
    if per_term:
        top_similar_terms = {}

        for input_term in term_list:
            try:
                input_embedding = encode_fn(input_term)

                # # Compute similarity (mock or real)
                # Mocked embeddings are NumPy arrays; real embeddings are tensors
                similarities = (
                    util.cos_sim(input_embedding, corpus_embeddings)
                    if not mock_encode
                    else np.dot(input_embedding, np.array(corpus_embeddings).T)
                )
                # Sort terms by similarity and retrieve the top N
                sorted_indices = np.argsort(similarities)[::-1]
                top_terms = [valid_corpus[i].lower() for i in sorted_indices[:topn]]

                # Include the original term if missing
                if input_term.lower() not in top_terms:
                    top_terms = [input_term.lower()] + top_terms[:topn - 1]

                top_similar_terms[input_term] = top_terms
                logging.info(f"Top terms for '{input_term}': {top_terms}")

            except Exception as e:
                logging.error(f"Error processing term '{input_term}': {e}")
                top_similar_terms[input_term] = [input_term]
        return top_similar_terms
    

    # Handle aggregated similarity (all input terms combined)
    else:
        # Compute combined embedding for the input list
        term_embeddings = [encode_fn(term) for term in term_list]
        if not term_embeddings:
            raise ValueError("No embeddings could be computed for the input list.")
        list_embedding = np.mean(term_embeddings, axis=0)  # Aggregate embeddings

        # Compute cosine similarities
        try:
            similarities = (
                util.cos_sim(list_embedding, corpus_embeddings).cpu().numpy()[0]
                if not mock_encode
                else np.dot(list_embedding, np.array(corpus_embeddings).T)
            )
            logging.info(f"First few similarities: {similarities[:10]}")
        except Exception as e:
            raise ValueError(f"Error computing similarities: {e}")
        
        if similarities is None or len(similarities) == 0:
            raise ValueError("No similarities could be computed.")
        
        sorted_indices = np.argsort(similarities)[::-1]
        top_similar_terms = [
            valid_corpus[i].lower()
            for i in sorted_indices[:topn]
            if len(set(valid_corpus[i].lower().split()).intersection(set([term for phrase in term_list for term in phrase.split()]))) == 0 # Corrected
            and len(valid_corpus[i].split()) <= 3
            and "and" not in valid_corpus[i].lower()
            and "or" not in valid_corpus[i].lower()
            and "in" not in valid_corpus[i].lower()
        ]
        logging.info(f"Top {topn} similar terms: {top_similar_terms}")
        return top_similar_terms
    
def expand_terms_for_query(terms, max_depth=1, topn=100, per_term = False):
    """
    Expand input terms using Wikipedia and semantic similarity.

    Args:
        terms (list of str): List of terms to expand.
        max_depth (int): Depth of Wikipedia traversal.
        topn (int): Number of top similar terms to return.

    Returns:
        list: Expanded list of terms.
    """
    logging.info(f"Expanding terms for: {terms}")
    corpus = fetch_wikipedia_corpus(terms, max_depth=max_depth, max_pages=400)
    if not corpus:
        logging.warning(f"The fetched corpus is empty for terms: {terms}.")
        return {term: [] for term in terms}  # Return an empty dictionary for each term

    # Generate top similar terms for each input term
    return generate_similar_terms(terms, model, corpus, topn, per_term=per_term)


def expand_and_save_to_json(mental_health_terms, epigenetic_terms, ethnographic_terms, socioeconomic_terms):
    """
    Expand terms for predefined categories and save them to a JSON file, ensuring all terms are lowercase.

    Args:
        mental_health_terms (list of str): Terms related to mental health.
        epigenetic_terms (list of str): Terms related to epigenetics.
        ethnographic_terms (dict of list): Terms related to race and ethnicity.
        socioeconomic_terms (list of str): Terms related to socioeconomic factors.
    """
    expanded_terms = {}

    # Expand Mental Health and Epigenetic terms with core term as keys
    expanded_terms["mental health terms"] = [
            expanded.lower() for expanded in expand_terms_for_query(mental_health_terms, topn=200,per_term=False)
        ]

    expanded_terms["epigenetic terms"] = [
            expanded.lower() for expanded in expand_terms_for_query(epigenetic_terms, topn=200,per_term=False)
        ]

    # Expand Socioeconomic Terms with core term as keys
    expanded_terms["socioeconomic terms"] = [
            expanded.lower() for expanded in expand_terms_for_query(socioeconomic_terms, topn=200, per_term=False)
        ]

    # Maintain the nested structure for ethnographic terms and normalize to lowercase
    expanded_terms["ethnographic terms"] = {
        category.lower(): [
            term.lower() for term in expand_terms_for_query(terms, topn=200, per_term=False)
        ]
        for category, terms in ethnographic_terms.items()
    }

    # Save expanded terms to a file
    with open("expanded_terms.json", "w", encoding="utf-8") as outf:
        json.dump(expanded_terms, outf, indent=4)
    logging.info("Expanded terms saved to './expanded_terms.json'.")


if __name__ == "__main__":
    # Pre-defined Core terms for querying
    mental_health_terms = [
        "depression", "bipolar", "PTSD", "anxiety", 
        "suicide", "generational trauma", "chronic stress", "mental disorder"
    ]
    epigenetic_terms = [
        "methylation", "demethylation", "CpG islands", "5mC", 
        "histone modification", "H3K27me3", "epigenetic", "epigenomics",
        "BDNF", "SLC6A4", "FKBP5", "OXTR", "stress response", 
        "HPA axis dysregulation", "childhood abuse"
    ]
    ethnographic_terms = {
        "african descent": [
            "african ethnicity", "african person", "african-american person", "black person", "black diaspora", "african-american mother"
        ],
        "latino/hispanic descent": [
            "latino person", "hispanic individual", "hispanic person", "hispanic ethnicity"
        ],
        "asian descent": [
            "asian person", "asian-American person", "east asian person", "south asian person", "asian ethnicity"
        ],
        "indigenous descent": [
            "indigenous person", "native american person", "first nations person", "indigenous peoples", "indigenous ethnicity"
        ],
        "arab descent": [
            "arab person", "middle-eastern individual", "muslim person", "arab-american person", "arab ethnicity"
        ],
        "european descent": [
            "european person", "caucasian person", "white person", "white american", "anglo-american", "european ethnicity"
        ]
    }
    socioeconomic_terms = [
    "low-income", "middle-income", "high-income", "poverty", "socioeconomic status", "illiteracy"
    ]

    # Generate the query
    expand_and_save_to_json(mental_health_terms, epigenetic_terms, ethnographic_terms, socioeconomic_terms)

