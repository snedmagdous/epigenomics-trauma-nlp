"""
preprocessing.py

Purpose:
- Processes raw PDF articles to prepare for downstream analysis.
- Cleans, tokenizes, and categorizes text based on predefined categories (e.g., Mental Health, Epigenetics).
- Extracts relevant terms and calculates statistics like co-occurrences, Jaccard similarity, etc.

Key Steps:
1. Load expanded terms for predefined categories from a JSON file (`top_similar_terms.json`).
2. Extract text content from articles (supports plain text and PDFs).
3. Preprocess text:
   - Clean and tokenize text.
   - Optionally use SciSpacy for domain-specific lemmatization and biomedical entity extraction.
4. Categorize text:
   - Match terms to predefined categories and count frequencies.
   - Compute co-occurrence and Jaccard similarity between categories.
5. Save results to a structured JSON file for further analysis.

Inputs:
- Directory containing raw articles (plain text or PDFs).
- JSON file of expanded terms (`top_similar_terms.json`) generated by `fetch.py`.

Outputs:
- JSON file (`preprocessed_articles.json`) containing:
  [
      {
          "paper_name": "example.pdf",
          "cleaned_text": "processed text here",
          "term_counts": {
              "Mental Health Terms": {"depression": 5, "anxiety": 3, ...},
              "Epigenetic Terms": {"methylation": 4, ...},
              ...
          },
          "jaccard_similarity": {
              "('Mental Health Terms', 'Epigenetic Terms')": 0.5,
              ...
          }
      },
      ...
  ]

How to Use:
- Run this script after `fetch.py` to preprocess raw article text or PDFs.
- The output JSON file can then be used in `topic_modeling.py` for deeper analysis.
"""

import os
import re
import json
import logging, chardet
import fitz  # PyMuPDF for PDF text extraction
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import download
from collections import defaultdict, Counter
import spacy

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Download NLTK dependencies
download("punkt")
download("stopwords")

# Load SciSpacy's biomedical model
# nlp = spacy.load("en_core_sci_lg")
nlp = spacy.load("en_core_web_sm")

# Get the current directory of the script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Paths relative to the script's directory
RAW_ARTICLES_DIR = os.path.normpath(r"C:/Users/snedm/Documents/Cornell/2024 Fall/CS 4701/cs4701/demo2/CAP_Epigenomics-Analysis_ma798_mmm443/CAP_Epigenomics-Analysis_ma798_mmm443/data/papers")
TOP_TERMS_FILE = os.path.join(BASE_DIR, "expanded_terms.json")  # Terms from fetch.py
OUTPUT_FILE = os.path.join(BASE_DIR, "preprocessed_articles.json")  # Adjust relative path

TOP_TERMS_FILE = os.path.normpath(TOP_TERMS_FILE)
OUTPUT_FILE = os.path.normpath(OUTPUT_FILE)

print(f"RAW_ARTICLES_DIR: {RAW_ARTICLES_DIR}")
print(f"TOP_TERMS_FILE: {TOP_TERMS_FILE}")
print(f"OUTPUT_FILE: {OUTPUT_FILE}")

# Load expanded terms (core + similar terms)s
try:
    with open(TOP_TERMS_FILE, "r", encoding="ascii", errors="replace") as file:
        expanded_terms = json.load(file)
    print("Successfully loaded expanded terms.")
except FileNotFoundError as e:
    print(f"Error: {e}")
    print(f"Ensure that the file exists at: {TOP_TERMS_FILE}")

def extract_text_from_pdf(pdf_path):
    """
    Extract text from PDF files.
    Args:
        pdf_path (str): File containing PDF content.
    Returns:
        list: List of dictionaries with extracted content.
    """
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        logging.info(f"Extracted text length for {pdf_path}: {len(text)}")
        return text.strip()
    except Exception as e:
        logging.error(f"Error extracting text from {pdf_path}: {e}", exc_info=True)
        return ""


def clean_text(text):
    """
    Perform basic text cleaning and tokenization.
    Args:
        text (str): Raw text to clean.
    Returns:
        str: Cleaned text.
    """
    return re.sub(r"[^\w\s]", " ", text.lower())


def lemmatize_and_process(text):
    """
    Lemmatize text using SciSpacy and preserve biomedical entities.
    Args:
        text (str): Cleaned text to process.
    Returns:
        list: Lemmatized tokens.
    """
    doc = nlp(text)
    return [token.lemma_ if token.ent_type_ == "" else token.text for token in doc]


def categorize_terms(text, expanded_terms):
    """
    Categorize text using expanded terms and count term frequencies.
    Args:
        text (str): Processed text to analyze.
        expanded_terms (dict): Dictionary of core terms and their expanded similar terms.
    Returns:
        dict: Term counts for each category.
    """
    term_counts = {category: Counter() for category in expanded_terms.keys()}
    tokens = text.split()
    for category, terms in expanded_terms.items():
        term_counts[category] = Counter(t for t in tokens if t in terms)
    return term_counts


def compute_co_occurrence(term_counts):
    """Compute a co-occurrence matrix for terms across categories."""
    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))
    for category1, terms1 in term_counts.items():
        for category2, terms2 in term_counts.items():
            if category1 != category2:
                for term1 in terms1:
                    for term2 in terms2:
                        co_occurrence_matrix[category1][term2] += terms1[term1]
    return co_occurrence_matrix


def preprocess_articles(input_dir=RAW_ARTICLES_DIR, expanded_terms=expanded_terms):
    """
    Process raw articles (text or PDFs) to clean, categorize, and calculate statistics.
    Args:
        input_dir (str): Directory containing raw articles.
        expanded_terms (dict): Dictionary with core terms and expanded similar terms.
    Returns:
        list: List of processed article metadata.
    """
    processed_articles = []
    global_term_counts = {category: Counter() for category in expanded_terms.keys()}
    global_relationships = defaultdict(lambda: {"terms": Counter(), "co_occurrence_count": 0, "jaccard_similarity": 0.0})
    MAX_NLP_LENGTH = 5000  # Limit for NLP processing

    for file_name in os.listdir(input_dir):
        file_path = os.path.join(input_dir, file_name)
        logging.info(f"Processing file: {file_name}")

        try: 
            if file_name.endswith(".pdf"):
                # Validate PDF
                try:
                    with fitz.open(file_path) as doc:
                        pass
                except Exception as e:
                    logging.error(f"Invalid or corrupted PDF: {file_name} - {e}")
                    continue
                raw_text = extract_text_from_pdf(file_path)
            else:
                with open(file_path, "r", encoding="ascii", errors="replace") as file:
                    raw_text = file.read()

            if not raw_text.strip():
                logging.warning(f"No extractable text found in file: {file_name}")
                continue

            # Clean, tokenize, and categorize terms
            cleaned_text = clean_text(raw_text)
            truncated_text = cleaned_text[:MAX_NLP_LENGTH]

            # Perform NLP processing with error handling
            try:
                doc = nlp(truncated_text)
            except Exception as e:
                logging.error(f"Error during NLP processing for {file_name}: {e}")
                continue

            lemmatized_text = " ".join([token.lemma_ if token.ent_type_ == "" else token.text for token in doc])
            term_counts = categorize_terms(lemmatized_text, expanded_terms)

            # Update global term counts
            for category, counts in term_counts.items():
                global_term_counts[category].update(counts)

            # Compute co-occurrence matrix
            co_occurrence_matrix = compute_co_occurrence(term_counts)

            # Add metadata (optional step based on text heuristics)
            disparity_metadata = {
                "ethnicity": next((term for term in expanded_terms["Ethnographic Terms"] if term in lemmatized_text), "Unknown"),
                "socioeconomic_status": next((term for term in expanded_terms["Socioeconomic Terms"] if term in lemmatized_text), "Unknown"),
            }

            logging.info(f"Disparity metadata for {file_name}: {disparity_metadata}")

            processed_articles.append({
                "paper_name": file_name,
                "cleaned_text": lemmatized_text,
                "term_counts": term_counts,
                "co_occurrence_matrix": co_occurrence_matrix,
                "disparity_metadata": disparity_metadata
            })

        except Exception as e:
            logging.error(f"Error processing file {file_name}: {e}", exc_info=True)
            continue

    # Calculate Jaccard similarities and global relationships
    for category1, terms1 in global_term_counts.items():
        for category2, terms2 in global_term_counts.items():
            if category1 != category2:
                intersection = sum((terms1 & terms2).values())
                union = sum((terms1 | terms2).values())
                global_relationships[(category1, category2)]["jaccard_similarity"] = intersection / union if union > 0 else 0.0
                global_relationships[(category1, category2)]["co_occurrence_count"] += intersection

    # Inline conversion of tuple keys to strings
    def convert_tuple_keys(d):
        """Recursively convert tuple keys to strings in a nested dictionary."""
        if not isinstance(d, dict):
            return d
        return {str(k): convert_tuple_keys(v) for k, v in d.items()}
    
    # Convert tuple keys to strings
    global_relationships_str_keys = convert_tuple_keys(global_relationships)

    # Save processed data
    with open(OUTPUT_FILE, "w") as output_file:
        json.dump({
            "papers": processed_articles,
            "global_summary": {
                "total_term_counts": {k: dict(v) for k, v in global_term_counts.items()},
                "top_relationships": global_relationships_str_keys
            }
        }, output_file, indent=4)

    logging.info(f"Processed articles saved to {OUTPUT_FILE}")
    return processed_articles


if __name__ == "__main__":
    processed_articles = preprocess_articles()
    logging.info(f"Processed {len(processed_articles)} articles successfully.")
